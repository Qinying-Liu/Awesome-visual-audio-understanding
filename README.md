# Awesome-visual-audio-understanding

## Methods
Collection of papers about video-audio understanding

- <span id = "1001">**[Video-LLaMA]**</span> | **Arxiv'2306** | DAMO Academy, Alibaba Group | Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding | [`[pdf]`](https://arxiv.org/pdf/2306.02858) | [`[code]`](https://github.com/DAMO-NLP-SG/Video-LLaMA)
- <span id = "1001">**[VideoLLaMA 2]**</span> | **Arxiv'2406** | DAMO Academy, Alibaba Group | VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs | [`[pdf]`](https://arxiv.org/abs/2406.07476) | [`[code]`](https://github.com/DAMO-NLP-SG/VideoLLaMA2)
- <span id = "1001">**[video-SALMONN]**</span> | **Arxiv'2406** | ByteDance | video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models | [`[pdf]`](https://arxiv.org/pdf/2406.15704) | [`[code]`](https://github.com/bytedance/SALMONN)
- <span id = "1001">**[VITA]**</span> | **Arxiv'2408** | Tencent Youtu | VITA: Towards Open-Source Interactive Omni Multimodal LLM | [`[pdf]`](https://arxiv.org/pdf/2408.05211) | [`[code]`](https://vita-home.github.io)
- <span id = "1001">**[VITA 1.5]**</span> | **Arxiv'2501** | Tencent Youtu | VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction | [`[pdf]`](https://arxiv.org/pdf/2501.01957) | [`[code]`](https://vita-home.github.io)
- <span id = "1001">**[Baichuan-Omni-1.5]**</span> | **Arxiv'2501** | Baichuan | Baichuan-Omni-1.5 Technical Report | [`[pdf]`](https://arxiv.org/pdf/2501.15368) | [`[code]`](https://github.com/baichuan-inc/Baichuan-Omni-1.5)
- <span id = "1001">**[Ola]**</span> | **Arxiv'2502** | Tencent HunYuan | Ola: Pushing the Frontiers of Omni-Modal Language Model | [`[pdf]`](https://arxiv.org/pdf/2502.04328) | [`[code]`](https://ola-omni.github.io)
- <span id = "1001">**[Qwen2.5-Omini]**</span> | **Arxiv'2503** | Qwen, Alibaba | Qwen2.5-Omni Technical Report | [`[pdf]`](https://arxiv.org/pdf/2503.20215) | [`[code]`](https://github.com/QwenLM/Qwen2.5-Omni)
- <span id = "1001">**[HumanOmniV2]**</span> | **Arxiv'2506** | Tongyi Lab, Alibaba Group | HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context | [`[pdf]`](https://arxiv.org/pdf/2506.21277) | [`[code]`](https://github.com/HumanMLLM/HumanOmniV2)
- <span id = "1001">**[video-SALMONN 2]**</span> | **Arxiv'2506** | ByteDance | video-SALMONN 2: Caption-Enhanced Audio-Visual Large Language Models | [`[pdf]`](https://arxiv.org/pdf/2506.15220) | [`[code]`](https://github.com/bytedance/video-SALMONN-2)
- <span id = "1001">**[Qwen3-Omini]**</span> | **Arxiv'2509** | Qwen, Alibaba | Qwen3-Omni Technical Report | [`[pdf]`](https://arxiv.org/pdf/2509.17765) | [`[code]`](https://github.com/QwenLM/Qwen3-Omni)
- <span id = "1001">**[InteractiveOmni]**</span> | **Arxiv'2510** | Sensetime | InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue | [`[pdf]`](https://arxiv.org/pdf/2510.13747) | [`[code]`](https://github.com/OpenSenseNova/InteractiveOmni)
- <span id = "1001">**[OmniVinci]**</span> | **Arxiv'2510** | NVIDIA | OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM | [`[pdf]`](https://arxiv.org/pdf/2510.15870) | [`[code]`](https://github.com/NVlabs/OmniVinci)
- <span id = "1001">**[Ming-Flash-Omni]**</span> | **Arxiv'2510** | Inclusion AI, Ant Group | Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation | [`[pdf]`](https://arxiv.org/abs/2510.24821) | [`[code]`](https://github.com/inclusionAI/Ming)
- <span id = "1001">**[LongCat-Flash-Omni]**</span> | **Arxiv'2511** | Meituan | LongCat-Flash-Omni Technical Report | [`[pdf]`](https://arxiv.org/pdf/2511.00279) | [`[code]`](https://github.com/meituan-longcat/LongCat-Flash-Omni)

## Benchmarks
- <span id = "1002">**[Video-MME]**</span> | **Arxiv'2405** | Nanjing University | Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis | [`[pdf]`](https://arxiv.org/pdf/2405.21075) | [`[code]`](https://github.com/MME-Benchmarks/Video-MME) 
- <span id = "1002">**[WorldSense]**</span> | **Arxiv'2502** | Xiaohongshu | WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs | [`[pdf]`](https://arxiv.org/pdf/2502.04326) | [`[code]`](https://github.com/JaaackHongggg/WorldSense) | FineVideo,MusicAVQA
- <span id = "1002">**[Daily-Omni]**</span> | **Arxiv'2505** |  Fudan University | Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities | [`[pdf]`](https://arxiv.org/abs/2505.17862) | [`[code]`](https://github.com/Lliar-liar/Daily-Omni) | AudioSet,Video-MME,FineVideo
- <span id = "1002">**[Video-Holmes]**</span> | **Arxiv'2505** | Tencent ARC Lab | Video-Holmes: Can MLLM Think like Holmes for Complex Video Reasoning? | [`[pdf]`](https://arxiv.org/pdf/2505.21374) | [`[code]`](https://github.com/TencentARC/Video-Holmes)
- <span id = "1002">**[OmniVideoBench]**</span> | **Arxiv'2510** | NJU-LINK Team | OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs | [`[pdf]`](https://arxiv.org/pdf/2510.10689) | [`[code]`]([https://github.com/TencentARC/Video-Holmes](https://github.com/NJU-LINK/OmniVideoBench))
